{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoSense at SemEval-2018 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "\n",
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            line = line.strip().split('\\t')\n",
    "            for i in range(1, 4):\n",
    "                line[i] = tokenize(line[i])\n",
    "            if mode == \"train\":\n",
    "                labels.append(emotion2label[line[4]])\n",
    "            conv = line[1:4]\n",
    "            conversations.append(conv)\n",
    "    if mode == \"train\":\n",
    "        return np.array(conversations), np.array(labels)\n",
    "    else:\n",
    "        return np.array(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train = preprocessData('./starterkitdata/train.txt', mode=\"train\")\n",
    "texts_dev, labels_dev = preprocessData('./starterkitdata/dev.txt', mode=\"train\")\n",
    "texts_test, labels_test = preprocessData('./starterkitdata/test.txt', mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(file):\n",
    "    embeddingsIndex = {}\n",
    "    dim = 0\n",
    "    with io.open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            dim = len(embeddingVector)\n",
    "    return embeddingsIndex, dim\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingMatrix[i] = embeddings.get(word)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings, dim = getEmbeddings('emosense.300d.txt')\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
    "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
    "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
    "labels_categorical_test = to_categorical(np.asarray(labels_test))\n",
    "\n",
    "\n",
    "def get_sequances(texts, sequence_length):\n",
    "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
    "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
    "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
    "    return message_first, message_second, message_third\n",
    "\n",
    "\n",
    "message_first_message_train, message_second_message_train, message_third_message_train = get_sequances(X_train, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_val, message_second_message_val, message_third_message_val = get_sequances(X_val, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequances(texts_dev, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_test, message_second_message_test, message_third_message_test = get_sequances(texts_test, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 24, 300)      197439000   input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNoise (None, 24, 300)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_5 (GaussianNoise (None, 24, 300)      0           embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_6 (GaussianNoise (None, 24, 300)      0           embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 128)          186880      gaussian_noise_4[0][0]           \n",
      "                                                                 gaussian_noise_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          186880      gaussian_noise_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_3[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30)           11550       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            124         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 197,824,434\n",
      "Trainable params: 385,434\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kutilities.callbacks import MetricsCallback, PlottingCallback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "metrics = {\n",
    "    \"f1_e\": (lambda y_test, y_pred:\n",
    "             f1_score(y_test, y_pred, average='micro',\n",
    "                      labels=[emotion2label['happy'],\n",
    "                              emotion2label['sad'],\n",
    "                              emotion2label['angry']\n",
    "                              ])),\n",
    "    \"precision_e\": (lambda y_test, y_pred:\n",
    "                    precision_score(y_test, y_pred, average='micro',\n",
    "                                    labels=[emotion2label['happy'],\n",
    "                                            emotion2label['sad'],\n",
    "                                            emotion2label['angry']\n",
    "                                            ])),\n",
    "    \"recoll_e\": (lambda y_test, y_pred:\n",
    "                 recall_score(y_test, y_pred, average='micro',\n",
    "                              labels=[emotion2label['happy'],\n",
    "                                      emotion2label['sad'],\n",
    "                                      emotion2label['angry']\n",
    "                                      ]))\n",
    "}\n",
    "\n",
    "_datasets = {}\n",
    "_datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n",
    "                    np.array(labels_categorical_dev)]\n",
    "_datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                    np.array(labels_categorical_val)]\n",
    "\n",
    "metrics_callback = MetricsCallback(datasets=_datasets, metrics=metrics)\n",
    "\n",
    "filepath = \"models/bidirectional_LSTM_best_weights_{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, save_weights_only=False,\n",
    "                             mode='auto', period=1)\n",
    "tensorboardCallback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/20\n",
      "24128/24128 [==============================] - 53s 2ms/step - loss: 0.7695 - acc: 0.6947 - val_loss: 0.4006 - val_acc: 0.8496\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.621032     0.529611  0.750600\n",
      "val  0.825791     0.859115  0.794956\n",
      "\n",
      "Epoch 2/20\n",
      "24128/24128 [==============================] - 44s 2ms/step - loss: 0.4336 - acc: 0.8359 - val_loss: 0.3197 - val_acc: 0.8854\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.655769     0.547352  0.817746\n",
      "val  0.871475     0.872333  0.870619\n",
      "\n",
      "Epoch 3/20\n",
      "24128/24128 [==============================] - 52s 2ms/step - loss: 0.3659 - acc: 0.8658 - val_loss: 0.3027 - val_acc: 0.8936\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.652416     0.532625  0.841727\n",
      "val  0.879769     0.861538  0.898788\n",
      "\n",
      "Epoch 4/20\n",
      "24128/24128 [==============================] - 43s 2ms/step - loss: 0.3280 - acc: 0.8792 - val_loss: 0.2764 - val_acc: 0.9017\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.672316     0.553488  0.856115\n",
      "val  0.891454     0.873116  0.910580\n",
      "\n",
      "Epoch 5/20\n",
      "24128/24128 [==============================] - 40s 2ms/step - loss: 0.2969 - acc: 0.8932 - val_loss: 0.2720 - val_acc: 0.9029\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.656335     0.529412  0.863309\n",
      "val  0.893469     0.865745  0.923027\n",
      "\n",
      "Epoch 6/20\n",
      "24128/24128 [==============================] - 36s 2ms/step - loss: 0.2771 - acc: 0.8980 - val_loss: 0.2566 - val_acc: 0.9087\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.696697     0.597938  0.834532\n",
      "val  0.897361     0.892707  0.902064\n",
      "\n",
      "Epoch 7/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2568 - acc: 0.9077 - val_loss: 0.2442 - val_acc: 0.9143\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.712385     0.621429  0.834532\n",
      "val  0.903755     0.904793  0.902719\n",
      "\n",
      "Epoch 8/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.2466 - acc: 0.9096 - val_loss: 0.2468 - val_acc: 0.9128\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.661142     0.536622  0.860911\n",
      "val  0.904656     0.881330  0.929250\n",
      "\n",
      "Epoch 9/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.2377 - acc: 0.9125 - val_loss: 0.2447 - val_acc: 0.9159\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.712412     0.614983  0.846523\n",
      "val  0.906398     0.900971  0.911890\n",
      "\n",
      "Epoch 10/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2192 - acc: 0.9204 - val_loss: 0.2484 - val_acc: 0.9151\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.730022     0.664047  0.810552\n",
      "val  0.904146     0.915687  0.892892\n",
      "\n",
      "Epoch 11/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.2095 - acc: 0.9250 - val_loss: 0.2261 - val_acc: 0.9193\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.703888     0.602389  0.846523\n",
      "val  0.910294     0.896979  0.924009\n",
      "\n",
      "Epoch 12/20\n",
      "24128/24128 [==============================] - 38s 2ms/step - loss: 0.1975 - acc: 0.9272 - val_loss: 0.2308 - val_acc: 0.9166\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.686424     0.570747  0.860911\n",
      "val  0.908684     0.887813  0.930560\n",
      "\n",
      "Epoch 13/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1913 - acc: 0.9303 - val_loss: 0.2577 - val_acc: 0.9120\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.729097     0.681250  0.784173\n",
      "val  0.901018     0.918652  0.884048\n",
      "\n",
      "Epoch 14/20\n",
      "24128/24128 [==============================] - 34s 1ms/step - loss: 0.1830 - acc: 0.9327 - val_loss: 0.2310 - val_acc: 0.9218\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.709948     0.630112  0.812950\n",
      "val  0.911654     0.912402  0.910907\n",
      "\n",
      "Epoch 15/20\n",
      "24128/24128 [==============================] - 39s 2ms/step - loss: 0.1714 - acc: 0.9361 - val_loss: 0.2313 - val_acc: 0.9221\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.715970     0.647287  0.800959\n",
      "val  0.912701     0.914502  0.910907\n",
      "\n",
      "Epoch 16/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.1646 - acc: 0.9387 - val_loss: 0.2442 - val_acc: 0.9181\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.688462     0.574639  0.858513\n",
      "val  0.909264     0.883025  0.937111\n",
      "\n",
      "Epoch 17/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.1571 - acc: 0.9417 - val_loss: 0.2341 - val_acc: 0.9211\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.696099     0.608618  0.812950\n",
      "val  0.912031     0.900671  0.923682\n",
      "\n",
      "Epoch 18/20\n",
      "24128/24128 [==============================] - 34s 1ms/step - loss: 0.1487 - acc: 0.9446 - val_loss: 0.2414 - val_acc: 0.9168\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.673684     0.560510  0.844125\n",
      "val  0.907381     0.883851  0.932198\n",
      "\n",
      "Epoch 19/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.1466 - acc: 0.9455 - val_loss: 0.2320 - val_acc: 0.9218\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.701538     0.612903  0.820144\n",
      "val  0.912366     0.904149  0.920734\n",
      "\n",
      "Epoch 20/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1350 - acc: 0.9502 - val_loss: 0.2482 - val_acc: 0.9213\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.694586     0.604982  0.815348\n",
      "val  0.911569     0.903215  0.920079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    callbacks=[metrics_callback, checkpoint, tensorboardCallback],\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=20,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"models/bidirectional_LSTM_best_weights_0010-0.9125.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7313432835820894\n",
      "precision_e 0.6583493282149712\n",
      "recoll_e 0.8225419664268585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      2338\n",
      "           1       0.64      0.76      0.69       142\n",
      "           2       0.70      0.84      0.76       125\n",
      "           3       0.65      0.87      0.74       150\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2755\n",
      "   macro avg       0.74      0.85      0.79      2755\n",
      "weighted avg       0.92      0.91      0.92      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7259100642398287\n",
      "precision_e 0.6544401544401545\n",
      "recoll_e 0.8149038461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      4677\n",
      "           1       0.65      0.74      0.70       284\n",
      "           2       0.66      0.86      0.75       250\n",
      "           3       0.65      0.85      0.73       298\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      5509\n",
      "   macro avg       0.73      0.84      0.78      5509\n",
      "weighted avg       0.92      0.91      0.91      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSubmissionFile(solution_path, test_data_path, predictions):\n",
    "    with io.open(solution_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "        with io.open(test_data_path, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "                \n",
    "                \n",
    "saveSubmissionFile('results.txt', './starterkitdata/test.txt', y_pred.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
