{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoSense at SemEval-2018 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "\n",
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            line = line.strip().split('\\t')\n",
    "            for i in range(1, 4):\n",
    "                line[i] = tokenize(line[i])\n",
    "            if mode == \"train\":\n",
    "                labels.append(emotion2label[line[4]])\n",
    "            conv = line[1:4]\n",
    "            conversations.append(conv)\n",
    "    if mode == \"train\":\n",
    "        return np.array(conversations), np.array(labels)\n",
    "    else:\n",
    "        return np.array(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './starterkitdata/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31222/4215005385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtexts_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./starterkitdata/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtexts_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./starterkitdata/dev.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtexts_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./starterkitdata/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31222/4193919178.py\u001b[0m in \u001b[0;36mpreprocessData\u001b[0;34m(dataFilePath, mode)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mfinput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './starterkitdata/train.txt'"
     ]
    }
   ],
   "source": [
    "texts_train, labels_train = preprocessData('./starterkitdata/train.txt', mode=\"train\")\n",
    "texts_dev, labels_dev = preprocessData('./starterkitdata/dev.txt', mode=\"train\")\n",
    "texts_test, labels_test = preprocessData('./starterkitdata/test.txt', mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(file):\n",
    "    embeddingsIndex = {}\n",
    "    dim = 0\n",
    "    with io.open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            dim = len(embeddingVector)\n",
    "    return embeddingsIndex, dim\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingMatrix[i] = embeddings.get(word)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 02:11:36.407041: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-24 02:11:36.407075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 658129 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings, dim = getEmbeddings('emosense.300d.txt')\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31222/246140867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlabels_categorical_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
    "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
    "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
    "labels_categorical_test = to_categorical(np.asarray(labels_test))\n",
    "\n",
    "\n",
    "def get_sequances(texts, sequence_length):\n",
    "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
    "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
    "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
    "    return message_first, message_second, message_third\n",
    "\n",
    "\n",
    "message_first_message_train, message_second_message_train, message_third_message_train = get_sequances(X_train, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_val, message_second_message_val, message_third_message_val = get_sequances(X_val, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequances(texts_dev, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_test, message_second_message_test, message_third_message_test = get_sequances(texts_test, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2303455524.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_31222/2303455524.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    pip install keras-utilities        weights=[embeddings_matrix],\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim, weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 24, 300)      197439000   ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNoise)  (None, 24, 300)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " gaussian_noise_1 (GaussianNois  (None, 24, 300)     0           ['embedding[1][0]']              \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " gaussian_noise_2 (GaussianNois  (None, 24, 300)     0           ['embedding[2][0]']              \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128)          186880      ['gaussian_noise[0][0]',         \n",
      "                                                                  'gaussian_noise_2[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128)         186880      ['gaussian_noise_1[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['bidirectional[0][0]',          \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional[1][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 384)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           11550       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            124         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 197,824,434\n",
      "Trainable params: 385,434\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'message_first_message_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31222/1751957413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m _datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n\u001b[0m\u001b[1;32m     28\u001b[0m                     np.array(labels_categorical_dev)]\n\u001b[1;32m     29\u001b[0m _datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'message_first_message_dev' is not defined"
     ]
    }
   ],
   "source": [
    "from kutilities.callbacks import MetricsCallback, PlottingCallback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "metrics = {\n",
    "    \"f1_e\": (lambda y_test, y_pred:\n",
    "             f1_score(y_test, y_pred, average='micro',\n",
    "                      labels=[emotion2label['happy'],\n",
    "                              emotion2label['sad'],\n",
    "                              emotion2label['angry']\n",
    "                              ])),\n",
    "    \"precision_e\": (lambda y_test, y_pred:\n",
    "                    precision_score(y_test, y_pred, average='micro',\n",
    "                                    labels=[emotion2label['happy'],\n",
    "                                            emotion2label['sad'],\n",
    "                                            emotion2label['angry']\n",
    "                                            ])),\n",
    "    \"recoll_e\": (lambda y_test, y_pred:\n",
    "                 recall_score(y_test, y_pred, average='micro',\n",
    "                              labels=[emotion2label['happy'],\n",
    "                                      emotion2label['sad'],\n",
    "                                      emotion2label['angry']\n",
    "                                      ]))\n",
    "}\n",
    "\n",
    "_datasets = {}\n",
    "_datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n",
    "                    np.array(labels_categorical_dev)]\n",
    "_datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                    np.array(labels_categorical_val)]\n",
    "\n",
    "metrics_callback = MetricsCallback(datasets=_datasets, metrics=metrics)\n",
    "\n",
    "filepath = \"models/bidirectional_LSTM_best_weights_{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, save_weights_only=False,\n",
    "                             mode='auto', period=1)\n",
    "tensorboardCallback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/20\n",
      "24128/24128 [==============================] - 53s 2ms/step - loss: 0.7695 - acc: 0.6947 - val_loss: 0.4006 - val_acc: 0.8496\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.621032     0.529611  0.750600\n",
      "val  0.825791     0.859115  0.794956\n",
      "\n",
      "Epoch 2/20\n",
      "24128/24128 [==============================] - 44s 2ms/step - loss: 0.4336 - acc: 0.8359 - val_loss: 0.3197 - val_acc: 0.8854\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.655769     0.547352  0.817746\n",
      "val  0.871475     0.872333  0.870619\n",
      "\n",
      "Epoch 3/20\n",
      "24128/24128 [==============================] - 52s 2ms/step - loss: 0.3659 - acc: 0.8658 - val_loss: 0.3027 - val_acc: 0.8936\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.652416     0.532625  0.841727\n",
      "val  0.879769     0.861538  0.898788\n",
      "\n",
      "Epoch 4/20\n",
      "24128/24128 [==============================] - 43s 2ms/step - loss: 0.3280 - acc: 0.8792 - val_loss: 0.2764 - val_acc: 0.9017\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.672316     0.553488  0.856115\n",
      "val  0.891454     0.873116  0.910580\n",
      "\n",
      "Epoch 5/20\n",
      "24128/24128 [==============================] - 40s 2ms/step - loss: 0.2969 - acc: 0.8932 - val_loss: 0.2720 - val_acc: 0.9029\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.656335     0.529412  0.863309\n",
      "val  0.893469     0.865745  0.923027\n",
      "\n",
      "Epoch 6/20\n",
      "24128/24128 [==============================] - 36s 2ms/step - loss: 0.2771 - acc: 0.8980 - val_loss: 0.2566 - val_acc: 0.9087\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.696697     0.597938  0.834532\n",
      "val  0.897361     0.892707  0.902064\n",
      "\n",
      "Epoch 7/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2568 - acc: 0.9077 - val_loss: 0.2442 - val_acc: 0.9143\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.712385     0.621429  0.834532\n",
      "val  0.903755     0.904793  0.902719\n",
      "\n",
      "Epoch 8/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.2466 - acc: 0.9096 - val_loss: 0.2468 - val_acc: 0.9128\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.661142     0.536622  0.860911\n",
      "val  0.904656     0.881330  0.929250\n",
      "\n",
      "Epoch 9/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.2377 - acc: 0.9125 - val_loss: 0.2447 - val_acc: 0.9159\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.712412     0.614983  0.846523\n",
      "val  0.906398     0.900971  0.911890\n",
      "\n",
      "Epoch 10/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2192 - acc: 0.9204 - val_loss: 0.2484 - val_acc: 0.9151\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.730022     0.664047  0.810552\n",
      "val  0.904146     0.915687  0.892892\n",
      "\n",
      "Epoch 11/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.2095 - acc: 0.9250 - val_loss: 0.2261 - val_acc: 0.9193\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.703888     0.602389  0.846523\n",
      "val  0.910294     0.896979  0.924009\n",
      "\n",
      "Epoch 12/20\n",
      "24128/24128 [==============================] - 38s 2ms/step - loss: 0.1975 - acc: 0.9272 - val_loss: 0.2308 - val_acc: 0.9166\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.686424     0.570747  0.860911\n",
      "val  0.908684     0.887813  0.930560\n",
      "\n",
      "Epoch 13/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1913 - acc: 0.9303 - val_loss: 0.2577 - val_acc: 0.9120\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.729097     0.681250  0.784173\n",
      "val  0.901018     0.918652  0.884048\n",
      "\n",
      "Epoch 14/20\n",
      "24128/24128 [==============================] - 34s 1ms/step - loss: 0.1830 - acc: 0.9327 - val_loss: 0.2310 - val_acc: 0.9218\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.709948     0.630112  0.812950\n",
      "val  0.911654     0.912402  0.910907\n",
      "\n",
      "Epoch 15/20\n",
      "24128/24128 [==============================] - 39s 2ms/step - loss: 0.1714 - acc: 0.9361 - val_loss: 0.2313 - val_acc: 0.9221\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.715970     0.647287  0.800959\n",
      "val  0.912701     0.914502  0.910907\n",
      "\n",
      "Epoch 16/20\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.1646 - acc: 0.9387 - val_loss: 0.2442 - val_acc: 0.9181\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.688462     0.574639  0.858513\n",
      "val  0.909264     0.883025  0.937111\n",
      "\n",
      "Epoch 17/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.1571 - acc: 0.9417 - val_loss: 0.2341 - val_acc: 0.9211\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.696099     0.608618  0.812950\n",
      "val  0.912031     0.900671  0.923682\n",
      "\n",
      "Epoch 18/20\n",
      "24128/24128 [==============================] - 34s 1ms/step - loss: 0.1487 - acc: 0.9446 - val_loss: 0.2414 - val_acc: 0.9168\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.673684     0.560510  0.844125\n",
      "val  0.907381     0.883851  0.932198\n",
      "\n",
      "Epoch 19/20\n",
      "24128/24128 [==============================] - 37s 2ms/step - loss: 0.1466 - acc: 0.9455 - val_loss: 0.2320 - val_acc: 0.9218\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.701538     0.612903  0.820144\n",
      "val  0.912366     0.904149  0.920734\n",
      "\n",
      "Epoch 20/20\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1350 - acc: 0.9502 - val_loss: 0.2482 - val_acc: 0.9213\n",
      "         f1_e  precision_e  recoll_e\n",
      "dev  0.694586     0.604982  0.815348\n",
      "val  0.911569     0.903215  0.920079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    callbacks=[metrics_callback, checkpoint, tensorboardCallback],\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=20,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31222/3288009416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/bidirectional_LSTM_best_weights_0010-0.9125.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"models/bidirectional_LSTM_best_weights_0010-0.9125.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31222/1687577146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage_first_message_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_second_message_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_third_message_dev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7313432835820894\n",
      "precision_e 0.6583493282149712\n",
      "recoll_e 0.8225419664268585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      2338\n",
      "           1       0.64      0.76      0.69       142\n",
      "           2       0.70      0.84      0.76       125\n",
      "           3       0.65      0.87      0.74       150\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2755\n",
      "   macro avg       0.74      0.85      0.79      2755\n",
      "weighted avg       0.92      0.91      0.92      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7259100642398287\n",
      "precision_e 0.6544401544401545\n",
      "recoll_e 0.8149038461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      4677\n",
      "           1       0.65      0.74      0.70       284\n",
      "           2       0.66      0.86      0.75       250\n",
      "           3       0.65      0.85      0.73       298\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      5509\n",
      "   macro avg       0.73      0.84      0.78      5509\n",
      "weighted avg       0.92      0.91      0.91      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29315/2343734432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msaveSubmissionFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./starterkitdata/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "def saveSubmissionFile(solution_path, test_data_path, predictions):\n",
    "    with io.open(solution_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "        with io.open(test_data_path, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "                \n",
    "                \n",
    "saveSubmissionFile('results.txt', './starterkitdata/test.txt', y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
